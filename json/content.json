{
	"personal": {
		"ids": [ "name", "profession", "bio" ],
		"name": "Matthew Hansen",
		"profession": "Game Programmer",
		"bio": "Thanks for taking the time to look at my portfolio! I am a professional software engineer with 10+ years of experience architecting high-performance software systems. The majority of my experience is in three areas: the video game industry on 2D and 3D games, DOD contracting on real-time vehicle simulations and maintenance programs, and working with personal clients on emerging technologies including eye-tracking, haptic feedback hardware, and AR/VR applications. My diverse experience has made me an expert at efficiently learning a wide variety of platforms and libraries with a keen eye for thoroughly examining specialized topics."
	},
	"projects": [
		{
			"ids": [ "t_archer", "d_archer", "tag_archer" ],
			"title": "Archer, P.I.",
			"description": "<p><b>Platforms:</b> Android, iOS</br><b>Team Size:</b> 7-10<br/><b>Studio:</b> Floyd County Productions / FX Networks<br/><b>Release:</b> 2017<br/><b>Engine/Tools:</b> Unity, Vuforia<br/><b>Languages:</b> C#, Java, Objective-C<br/><b>Role:</b> Programmer<br/><b>Time on project:</b> 6 months</p><h3>My Contributions</h3><p><ul><li>Gameplay</li><li>UI & UX</li><li>Achievements</li><li>Permissions</li><li>AR image recognition</li></ul></p><h3>Overview</h3><p>Archer, P.I. is an Augmented Reality app designed to interface directly with the TV show, allowing the player to literally grab an item from an episode and interact with it while immersed in rich secondary narrative that compliments developments in the show. The app seamlessly weaves animated video storylines with graphical hints while leveraging game mechanics to surprise and delight fans as they solve a new case for each episode of the show.</p><video poster=\"img/archer/video-poster.jpg\" preload=\"auto\" controls><source src=\"vid/archer-award-pitch.mp4\" type=\"video/mp4\"></video><h3>Core Gameplay & Augmented Reality</h3><p>I worked on a variety of gameplay props and puzzles by setting up Augmented Reality images in our database to ensure each item, or target, was recognized by the phone’s camera and put into the player’s inventory. When a player tapped on the item in their inventory, it would bring them to a new scene that showcased the item and enabled interaction with other clues.</p><p><img src=\"img/archer/dog-ar-target.png\"/></p><p>One of the gags I implemented was Archer’s safe. The player would acquire it by scanning the safe from Episode 1 and could inspect it in the app. Once in the app the player could then rotate the combination dial in any direction to begin the process of entering an excessively long code that Archer claimed was changed daily.</p><p><img src=\"img/archer/archer-pi-safe-code.png\"/></p><p>I simulated this to work in the same manner as a standard combination lock, including the ability to reset a combination in case the player entered an incorrect number. I also tuned haptic feedback from the phone’s vibration settings to make it feel as if you were actually interacting with a real lock.</p><p><img src=\"img/archer/archer-safe-dial.png\"/></p><p>After a few successful entries, or enough failures, the puzzle was designed to break and the dial would pop off, revealing the contents inside - a clue for the next part of the puzzle.</p><p><img src=\"img/archer/archer-safe-broken.png\"/></p><h3>OS Permissions</h3><p>I worked on the permission system between our game engine - Unity, the AR middleware - Vuforia, and operating systems - Android and iOS.</p><p>This was a bit of a challenge because we needed to request camera permissions from our users before the engine initialized, but depending on the operating system and its specific version, the user may not get a permission prompt until the app is launched. The problem was magnified, particularly on Android devices, due to market fragmentation in 2017.</p><img src=\"img/archer/android-dashboard.png\"/><p>Each version of the Android operating system had a different philosophy on how to manage permissions and privacy settings. For example, some would allow the use of a camera only when the app was open, some expected 24/7 access to the permission, and others could be on a case by case basis.</p><p>The solution I chose was to create a native Android activity in Java to request permissions from the user in the manner appropriate for their OS version, as well as user preference in more modern versions of Android. This activity then launched our engine, initialized our middleware, loaded our assets, and then brought the user to the title screen of our app.</p><img src=\"img/archer/permission-flowchart.png\"/><p>On the iOS side, device fragmentation and permission standards were more consistent - making it much easier to manage, but we still needed to make sure permissions remained active while our systems initialization was in process. I used a similar solution to develop a native iOS app for phones and tablets using Objective-C and integrated the OSX build process into our team’s existing deployment pipeline for convenient iterative builds on each platform. This allowed us to keep our testing team up-to-date with new gameplay features, performance metrics, asset quality checks, and permission verification across all supported devices.</p>",
			"tags": [ "Unity", "C#", "Augmented Reality", "Gameplay", "User Interface", "iOS / Android", "Database", "Achievements" ]
		},
		{
			"ids": [ "t_chan", "d_chan", "tag_chan" ],
			"title": "The Channeler",
			"description": "<p><b>Platform:</b> PC</br><b>Team Size:</b> 12</br><b>Release:</b> 2016</br><b>Engine/Tools:</b> Unreal Engine, Tobii EyeX</br><b>Languages:</b> C++, Blueprint, Python</br><b>Role:</b> Programmer</br><b>Time on project:</b> 16 months</p><h3>My Contributions</h3><p><ul><li>Gameplay puzzles</li><li>Eye-tracking research and implementation</li><li>Playtesting & data analytics</li><li>Adding controller support to gameplay and UI navigation</li><li>Build automation and installation</li></ul></p><h3>Overview</h3><p>The Channeler is an experimental PC game that strives to create a unique and immersive experience that utilizes Tobii EyeX eye-tracking hardware. Flex your \"third eye\" in this kooky city of spirits, where the denizens are plagued by mysterious disappearances.</p><h3>Eye-Tracking Challenges</h3><p>I worked with the eye-tracker extensively during this project and spent a significant portion observing how our users interacted with the concept of eye-tracking being the primary form of interaction with the game. One of the challenges that I faced was stabalizing jitter with the reticle. A reticle, or crosshair, was needed in our project because the user would control the majority of their gameplay as well as the camera movement, with their eyes.<img src=\"img/channeler/ChannelerEyeCamera.gif\"</p>",
			"tags": [ "Unreal Engine 4", "C++", "Blueprint", "Tobii EyeX", "Artificial Intelligence", "Gameplay / Prototyping", "Analytics", "Build Pipeline" ]
		},
		{
			"ids": [ "t_brain", "d_brain", "tag_brain" ],
			"title": "Brain Maxx",
			"description": "<p><b>Platforms:</b> PC</br><b>Team Size:</b> 9<br/><b>Release:</b> 2016<br/><b>Engine/Tools:</b> Unity, EEGer<br/><b>Languages:</b> C#, Python<br/><b>Role:</b> Programmer<br/><b>Time on project:</b> 4 months</p><h3>My Contributions</h3><ul><li>Gameplay</li><li>Audio tuning and tooling</li><li>EEG data management</li></ul><h3>Overview</h3><p>Brain Maxx is a game that relies on gathering neurofeedback data in real-time in the form of EEG waves and encourages the player to enter a focused state. The player is initially represented as a small object traveling down a cylindrical space. By monitoring spontaneous electrical activity while engaged in the game, we reward the player when they remain in their focused state by accumulating additional objects that form into a ship. If the player gets either too relaxed or too energized, they will fall out of this reward state. If a player is unable to quickly re-enter a focused state the ship will begin to disintegrate and audio will fade. We hope to gain a better understanding of how games can impact a user's level of focus in real-time and see if repetitive exposure can assist in consciously managing it.</p>",
			"tags": [ "C#", "Unity", "Muse", "EEGer", "Gameplay", "Audio", "Experimental" ]
		},
		{
			"ids": [ "t_engine", "d_engine", "tag_engine" ],
			"title": "Data-Driven Game Engine",
			"description": "<p><b>Platforms:</b> Any</br><b>Team Size:</b> Solo<br/><b>Release:</b> 2016<br/><b>Engine/Tools:</b> Custom, OpenGL, DirectX<br/><b>Languages:</b> C++ (engine), XML (scripting)<br/><b>Role:</b> Programmer<br/><b>Time on project:</b> 8 months</p><h3>Data Structures</h3><p>I have been writing a custom, cross-platform, data-driven game engine from scratch in C++. This includes re-creating fundamental data structures optimized for performance, such as:</p><ul><li>singly-linked list</li><li>vector</li><li>stack</li><li>hashmap</li></ul><h3>Scripted System</h3><p>In addition to compile-time containers, I have been working on creating a script-like system to construct custom data types during run-time. The components that make up this system are datums - storing either individual data or arrays of homogeneous data with Run-Time Type Information (RTTI) support, scopes - which encase datums or other scopes with an associated name, and attributes - which link a script-like language, in this case XML, to native C++ code. And finally, an event-driven XML parsing system which is used for the creation of custom data systems at run-time.</p>",
			"tags": [ "C++", "Data Structures", "RTTI", "Data Driven System", "Chain of Responsibility", "XML Parsing", "Factory Pattern" ]
		},
		{
			"ids": [ "t_breakout", "d_breakout", "tag_breakout" ],
			"title": "Motorola 68k Breakout",
			"description": "<p><b>Platforms:</b> PC</br><b>Team Size:</b> Solo<br/><b>Release:</b> 2015<br/><b>Engine:</b> Custom<br/><b>Languages:</b> Motorola 68000 Assembly<br/><b>Role:</b> Programmer<br/><b>Time on project:</b> 4 months</p><h3>Overview</h3><p>For this project I recreated the classic game of Breakout in a 32-bit assembly language - Motorola 68000. There were a few areas I needed to address to make this game work:</p><ul><li>Rendering</li><li>Physics</li><li>Gameplay</li></ul><h3>Rendering</h3><p>Rendering on the CPU in particular was too slow to be able to redraw a full bitmap background on the screen, even when swapping buffers in a double buffered system. In order to work around this limitation, I calculated an invalid rectangle each frame where an entity had moved from. This allowed me to renderer only areas of the screen where another sprite was overlapping or part of a background image that had been occupied by an entity from the previous frame.</p><h3>Gameplay & Physics</h3><p>One of the largest challenges involved optimizing the application to run through a CPU simulator in a Windows environment. Some of these challenges entailed using fixed point notation because the hardware did not support floating point operations, creating a 7-segment LED for the life counter to minimize loading additional sprite and fonts assets, and maintaining a user input buffer in the cache to ensure the game was immediately responsive to the player’s input.</p>",
			"tags": [ "Assembly", "Motorola 68000", "Low-level Optimization", "Fixed Point" ]
		},
		{
			"ids": [ "t_nyah", "d_nyah", "tag_nyah" ],
			"title": "Nyah",
			"description": "<p><b>Platforms:</b> PC, Xbox</br><b>Team Size:</b> 2<br/><b>Release:</b> 2015<br/><b>Engine/Tools:</b> Custom, OpenGL, DirectX<br/><b>Languages:</b> C, C++<br/><b>Role:</b> Programmer<br/><b>Time on project:</b> 4 months</p><h3>Overview</h3><p>A PC game developed by a team of two. Originally written in C++ and OpenGL, it was later ported to the original Xbox. Some of the more notable technical challenges included converting from OpenGL to DirectX, handling new input devices, and managing requirements for audio and graphic assets across platforms.</p>",
			"tags": [ "C/C++", "Cross Platform", "PC", "Xbox", "OpenGL", "DirectX" ]
		}
	]
}